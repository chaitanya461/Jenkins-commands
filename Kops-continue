ENV VARIABLES:

It is a way to pass configuration information to containers running within pods. To set Env vars it include envFrom field in the configuration file.

ENV: Allows you to set environment variables for a container, specifying a value directly for each variable throug CLI/ Command prompt

ENVFROM: PASSING Variables FROM FILE 2 Types, configmaps and secrets

CONFIGMAPS & SECRETS:

D

It is used to store the data in key-value pair, files, or command-line arguments that can be used by pods, containers in cluster.

But the data should be non-confidential. It does not provide security and encryption.

If you want to provide Enryption use Secrets in K8S.

Limit of the configmap is only 1MB

But if you want to store more than 1MB configmap data mount volume or use a separate database or a file service

Example with ENV:

kubectl create deploy newdb --image=mariadb    [ having mariadb]

[This will create a deployment called newdb with single pod

kubectl get pods

kubectl logs newdb-794dd57dbc-tr7s9 [It is crashed because we haven't specified the passsowrd for MariaDB]

kubectl set env deploy newdb MYSQL_ROOT_PASSWORD-root123456

kubectl get pods [ Now it will be in running state, but we are passing password directly from command]

kubectl delete deploy newdb

kubectl create deploy newdb-image-mariadb

kubectl get pods [This will fail because no env variable]

vi var
MYSQL_ROOT_PASSWORD=root123456
MYSQL_ROOT_User=root123456

kubectl create cm dbvars --from-env-file=vars

kubectl set env deploy newdb --from=configmaps/dbvars

first create secrets, 2 ways to create, from CLI or from File

kubectl create secret generic password --from-literal=ROOT_PASSWORD=reyaz123 (from cli) value literal meaning direct

kubectl create secret generic my-secret-from-env-file=vars (from file)

kubectl get secrets

kubectl describe secret my-secret [Name of the secret is my-secret]

kubectl set env deploy newdb --from=secrets/password

kubectl get pods [This will fail because we have not mention the MYSQL_ prefix]

kubectl set env deploy newdb --from=secret/password --prefix=MYSQL_

without passing prefix we cant make the pod running status

TO SEE SECRETS:

kubectl get secrets password -o yaml

echo -n "LKJSKFHJHi" | base64 -d

or

echo -n "LKJSKFHJHi" base64-decode

kubectl delete deploy newdb

container writes log data to a shared volume, and the sidecar container serves these logs over HTTP using nginx

vi sidecar.yml

apiVersion: v1
kind: Pod
metadata:
  name: log-aggregator-pod
spec:
  containers:
  - name: app-container
    image: alpine
    command: ["/bin/sh", "-c"]
    args: ["while true; do date >> /var/log/app.log; sleep 5; done"]
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log
  - name: sidecar-container
    image: nginx
    volumeMounts:
    - name: shared-logs
      mountPath: /usr/share/nginx/html
  volumes:
  - name: shared-logs
    emptyDir: {}

----------_--------------------------------------------------------------

INGRESS:

Ingress is a service to expose application, but we already have cluster ip, node port and load balancer, let see

Ingress helps to expose HTTP and HTTPS routes from outside of the Cluster

Ingress supports Host based routing and path based routing

ingress supports load balancing and SSL termination

IT redirect the incoming requests to the right services based on the web url or path in the address

ingress provides encryption feature and helps to balance the load of the applications

Explain Host based and Path based

Host Based Routing: ex: boom.com, web.boom.com, admin.boom.com

Path based routing: boom.com/hello, boom.com/admin, Paytm.com/movies, Paytm.com/recharge etc

but services like load balancer, cluster ip, node port etc donest have these features

General load balancer routes the traffic based on ports and cant handle URL based routing

kubectl get ing-> shows ingress service, no ingress service

To install ingress, firstly we have to install nginx ingress controller:
command:

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.3.0/deploy/static/provider/cloud/deploy.yaml

kubectl get pods

kubectl get deploy

kubectl get svc

kubectl get ingress

kubectl get service



vi httpd.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd
  labels:
    app: httpd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: httpd
  template:
    metadata:
      labels:
        app: httpd
    spec:
      containers:
      - name: httpd
        image: httpd
        ports:
        - containerPort: 80
        env:
        - name: TITLE
          value: "APACHE APP2"
---
apiVersion: v1
kind: Service
metadata:
  name: httpd
spec:
  type: ClusterIP
  ports:
  - port: 80
  selector:
    app: httpd

vi nginx.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        env:
        - name: TITLE
          value: "NGINX APP1"
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  type: ClusterIP
  ports:
  - port: 80
  selector:
    app: nginx

vi ingrees.yml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: k8s-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /nginx(/|$)(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: nginx
            port:
              number: 80
      - path: /httpd(/|$)(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: httpd
            port:
              number: 80
      - path: /(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: nginx
            port:
              number: 80

---------------------------------------_---------------------------------------------

PODS SCHEDULING:

In Kubernetes, Node Selector, Node Affinity, Taints and Tolerations and are mechanisms that influence how Pods are scheduled onto Nodes within a cluster.

Node Selector

Node Affinity

Taints and Tolerations

1. Node Selector:

NodeSelector is the simplest form of node selection constraint, allowing Pods to be scheduled only on Nodes with specific labels. By specifying a NodeSelector in a Pod's specification, you can ensure that the Pod runs only on Nodes that match the given label criteria.

NodeSelector: Use when you have simple, specific constraints for Pod placement based on Node labels

In the below manifest file, we are creating 2 pods and those pods should be scheduled in ib-node labeled node.

vi nodeselector.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 2
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      nodeSelector:
        node-name: ib-node
      containers:
      - name: cont1
        image: nginx

kubectl get nodes

kubectl edit node node-name

kubectl apply -f nodeselector.yml

kubectl get pods

--Pods are not getting created because there is no label to the node called node-name: th-node

kubectl describe pod ib-deployment-7784c9bfb5-8s18d

--Warning FailedScheduling 2m40s default-scheduler 0/3 nodes are available: 1 node(s) had untolerated taint (node-role.kubernetes.io/control-plane:), 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes. are available: 3 Preemption is not helpful for scheduling.

kubectl get nodes

kubectl edit node 1-043783332483bf9f8

--under labels add -node-name: ib-node / We can use command also "kubectl label nodes 1-043783332483bf9f8

node-name-ib-node

kubectl get pods  --Now pods are running

--we are forcing kube-schedular to schedule pod on particular node. Its hard match. If it doesn't match, Kube scheduler will not schedule the pod so pod will be in pending state

kubectl delete deploy ib-deployment

2: Node Affinity:

Node Affinity is a more expressive way to specify rules about the placement of pods relative to nodes' labels. It allows you to specify rules that apply only if certain conditions are met. Same as Node Selector but this has flexible way, if matches do it, if not schedule pod on any another node.

Two types

1.Preferred during scheduling Ignore during execution (soft rules) good if happen

2. Required during scheduling Ignore during execution (hard rules) Must happen Same as NodeSelector

The node affinity syntax supports the following operators: In, NotIn, Exists, DoesNotExist, Gt, Lt, etc.

Preferred:

vi preferred.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
              - key: node-name
                operator: In
                values:
                - mb-node
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80

Required:

vi required.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-name
                operator: In
                values:
                - mb-node
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

TAINTS and TOLERANCE:

--In Kubernetes, taints and tolerations work together to control the scheduling of Pods onto Nodes. Taints are applied to Nodes to prevent certain Pods from being scheduled on them, while tolerations are applied to Pods to allow them to be scheduled on Nodes with matching taints.

Example Scenario: Dedicated Nodes for Specific Workloads

--Suppose you have a Kubernetes cluster with Nodes equipped with specialized hardware, such as GPUs, intended exclusively for machine learning workloads. To ensure that only Pods requiring GPU resources are scheduled on these Nodes, you can use taints and tolerations. I

kubectl edit node 1-0e6b67bf1b00383be [Edit control plane, this master node has taint no schedule, thats the reason master node will not have pods]

--Apply Taint to GPU Nodes

--First, taint the GPU Nodes to repel/force Pods that do not require GPU resources:

kubectl taint nodes 1-0333b24c25bf4868b hardware=gpu:NoSchedule

--This command adds a taint with key hardware, value gpu, and effect NoSchedule to the specified Node. As a result, Pods without a matching toleration will not be scheduled on this Node.

--Lets test:

vi deploy.yml
same deploy yml file

vi toleration:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
        - name: conti
          image: reyadocker/internetbankingrepo:latest
      tolerations:
        - key: "hardware"
          operator: "Equal"
          value: "gpu"
          effect: "NoSchedule"

--In this Pod specification, the toleration matches the taint applied to the GPU Nodes, permitting the Pod scheduled on those Nodes.

Key Points:

--Taints are applied to Nodes to repel certain Pods. They consist of a key, value, and effect (NoSchedule, PreferNoSchedule, or NoExecute).

--Tolerations are applied to Pods to allow them to be scheduled on Nodes with matching taints. They must match the key, value, and effect of the taint to be effective.

--Pod Affinity

--Pod affinity allows users to specify which pods a pod should or should not be scheduled with based on labels. For example, you can use pod affinity to specify that a pod should be scheduled on the same node as other pods with a specific label, such as app-database.
----------------------------------------------------

mini project for mongobd & mongoexpress
vi mongo-stack.yaml

# 1. Secret for MongoDB Credentials
apiVersion: v1
kind: Secret
metadata:
  name: mongodb-secret
type: Opaque
data:
  # Values must be base64 encoded strings
  # Run `echo -n 'your_username' | base64` and `echo -n 'your_password' | base64`
  mongo-root-username: YWRtaW4= # base64 encoded for "admin"
  mongo-root-password: cGFzc3dvcmQ= # base64 encoded for "password"
---

# 2. ConfigMap for MongoDB connection URL
apiVersion: v1
kind: ConfigMap
metadata:
  name: mongodb-configmap
data:
  database_url: mongodb-service # This must match the name of the MongoDB internal Service below
---

# 3. Persistent Volume Claim (PVC) for MongoDB data
# This requests storage from your Kubernetes cluster's default StorageClass
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc
spec:
  accessModes:
    - ReadWriteOnce # This access mode means the volume can be mounted as read-write by a single Node
  resources:
    requests:
      storage: 1Gi # Request 1 Gigabyte of storage, adjust as needed
---

# 4. MongoDB Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb-deployment
  labels:
    app: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo:latest # Use a stable version for production, e.g., mongo:6.0
        ports:
        - containerPort: 27017 # Default MongoDB port
        env:
        # Inject credentials from the Secret
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-password
        volumeMounts:
        - name: mongodb-data
          mountPath: /data/db # Default data path for MongoDB
      volumes:
      - name: mongodb-data
        persistentVolumeClaim:
          claimName: mongodb-pvc
---

# 5. MongoDB Internal Service (ClusterIP)
apiVersion: v1
kind: Service
metadata:
  name: mongodb-service # This name is used in the ConfigMap and by the Mongo Express app
spec:
  selector:
    app: mongodb
  ports:
  - protocol: TCP
    port: 27017
    targetPort: 27017
  type: ClusterIP # Only accessible within the Kubernetes cluster
---

# 6. Mongo Express Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-express
  labels:
    app: mongo-express
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-express
  template:
    metadata:
      labels:
        app: mongo-express
    spec:
      containers:
      - name: mongo-express
        image: mongo-express:latest # Official Mongo Express image
        ports:
        - containerPort: 8081 # Default Mongo Express web port
        env:
        # Inject connection details from Secret and ConfigMap
        - name: ME_CONFIG_MONGODB_ADMINUSERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
        - name: ME_CONFIG_MONGODB_ADMINPASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-password
        - name: ME_CONFIG_MONGODB_SERVER
          valueFrom:
            configMapKeyRef:
              name: mongodb-configmap
              key: database_url
---

# 7. Mongo Express External Service (LoadBalancer)
apiVersion: v1
kind: Service
metadata:
  name: mongo-express-service
spec:
  selector:
    app: mongo-express
  type: LoadBalancer # Exposes the service externally via a cloud LoadBalancer IP
  ports:
  - protocol: TCP
    port: 80 # The port the service exposes
    targetPort: 8081 # The container port to forward traffic to


kubectl apply -f mongo-stack.yaml
kubectl get all



