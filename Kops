curl -LO "https://dl.k8s.io/release/$(curl Ls https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" 

wget https://github.com/kubernetes/kops/releases/download/v1.25.0/kops-linux-amd64

chmod +x kops-linux-amd64 kubectl

mv kubectl /usr/local/bin/kubectl

mv kops-linux-amd64 /usr/local/bin/kops

copy paste all at a time if required

vi.bashrc

export PATH=$PATH:/usr/local/bin/

wq!

source .bashrc

aws s3api create-bucket-bucket reyaz-kops-testbkt123.k8s.local-region ap-south-1-create-bucket-configuration

LocationConstraint-ap-south-1

aws s3api put-bucket-versioning-bucket reyaz-kops-testbkt123.k8s.local-region ap-south-1-versioning-configuration Status-Enabled

export KOPS_STATE_STORE=s3://reyaz-kops-testbkt123.k8s.local

kops create cluster-name reyaz.k8s.local-zones ap-south-la-master-count=1--master-size t2.medium-node-count-2-node-size t2.micro

kops update cluster-name reyaz.k8s.local-yes--admin

kops validate cluster --wait 10m
--------+++----------------------------------------
For Future Reference

Suggestions:

* list clusters with: kops get cluster

edit this cluster with: kops edit cluster reyaz.k8s.local

edit your node instance group: kops edit ig-name-reyaz.k8s.local nodes-ap-south-la

edit your master instance group: kops edit ig-name-reyaz.k8s.local master-ap-south-la

-----------------------------------------------------

kubectl get namespace/ns

kubectl get pods

kubectl get namespace/ns

kubectl get pods

kubectl describe pod [By default pods are created in default namespace]

kubectl get pods -A [This will list all pods from all namespaces]

kubectl get pods-all-namespaces [This can also be used to get all pods from all namespaces]

kubectl get pods -n default [This will list pods wchich is in default namespace]

kubectl get pods -n kube-node-release no pods here

kubectl get pods -n kube-public no pods here

kubectl get pods -n kube-system [all kubeproxy, apiserver, controller

---------------------------------------------------------

Scale Out Worker Nodes :

kops edit ig-name-reyaz.k8s.local nodes-ap-south-la --> max size 4, min size 4

kops update cluster-name reyaz.k8s.local-yes--admin

kops rolling-update cluster --yes

See in AWS Console, 2 additional worker nodes got created

kubectl get nodes. [takes time to show]

Scalle out Master Nodes:

kops edit ig-name-reyaz.k8s.local master-ap-south-la --> max size 2, min size 2

kops update cluster-name reyaz.k8s.local-yes-admin

kops rolling-update cluster --yes

See in AWS Console, 1 additional Master nodes got created

kubectl get nodes [takes time to show]

_--------------------------------------------------------

IF ERROR

Step 3: If Kubelet is Installed but Not Found in Systemd If kubelet is installed but still not found in systemd, try reloading systemd:

sudo systemctl daemon-reload

sudo systemctl restart kubelet

If the issue persists, manually add the systemd service file:

sudo nano /etc/systemd/system/kubelet.service

[Service]
ExecStart=/usr/bin/kubelet Restart always StartLimitInterval=0
RestartSec=10
[Install]
WantedBy=multi-user.target
Then reload and start:

sudo systemctl daemon-reload sudo systemctl enable kubelet sudo systemctl restart kubelet

Step 4: Check Logs for Errors If kubelet is still failing, check logs:

journalctl -u kubelet -f

----------------------------------------------------------

Lets create a pods in namespaces

kubectl get ns

kubectl create ns dev

kubectl get ns

Currently I am in default namespace, how to check

kubectl config view [ output doesn't show namespace, if you don't see any namespace, this is default]

kubectl config set-context-current-namespace dev [This is use to switch to move from another namespace]

kubectl config view [ Now this output show dev namespace]

kubectl get pods [Now you are in dev namespace, now you cannot see any pods of other namespace] I

create a new pods in this namespace

kubectl run devi-image nginx

kubectl run dev2-image nginx

kubectl run dev3image nginx

kubectl get pods

----+++-----------------------------------------------------

Create a namespace in kubernetes :

kubectl create ns dev [use if already exits]

kubectl config set-context-current-namespace-dev

First create a sample serviceaccount here in kubernetes called jack

Create a Serviceaccount--> Create a ROLE(give permissions)--> Role Binding

vi serviceaccount.yml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: jack
  namespace: dev


kubectl create -f serviceaccount.yml

Create a Role within the dev namespace that allows the user jack to perform certain actions on pods:

vi role.yml

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: dev-pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]

kubectl create -f role.yml

Since pods belong to the core API group, we use apiGroups: [""]

Next, bind the pod-reader Role to a user or service account. For simplicity, we'll bind it to a service

account jack:

This RoleBinding binds the pod-reader Role to the jack user, allowing that user to get, list, and watch pods in th dev namespace.

vi rolebinding.yml

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: dev
subjects:
- kind: User
  name: jack
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: dev-pod-reader
  apiGroup: rbac.authorization.k8s.io

kubectl create f rolebinding.yml

Verifying Access

To verify that the jack user has the correct permissions, you can impersonate the user using kubectl:

kubectl auth can-i list pods --namespace=dev --as=jack [answer yes]

kubectl auth can-i create pods --namespace=dev --as=jack [answer no]

kubectl get roles -n dev [List all roles in a namespace:

kubectl describe rolebinding read-pods -n dev

----+++-----------------------------------------------------

Key Concepts of Kubernetes Services:

ClusterIP:

This is the default type of Service. It exposes the Service on a cluster-internal IP. Other services within the same Kubernetes cluster can access the Service, but it is not accessible from outside the cluster.

This creates a connection using an internal Cluster IP address and a Port.

NodePort:

This type of Service exposes the Service on each Node's IP at a static port. A NodePort Service is accessible from outside the cluster by hitting the <NodeIP>: <NodePort>.

When a NodePort is created, kube-proxy exposes a port in the range 30000-32767:

TargetPort:

Pod Container port. Pod's container listens on applicationn port Ex: 80 or 8080, if you dont use this line, K8s will assign default 80 port

kubectl get svc [By default kuberenets will create a default svc]

Let us create a new service

vi service.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1 # Added hyphen list indicator
        image: trainerreyaz/ib-image:latest
---
apiVersion: v1 # Corrected 'apiVersion: v1' placement
kind: Service
metadata:
  name: ib-service
spec:
  type: ClusterIP # Corrected 'ClusterIPI' to 'ClusterIP'
  selector:
    app: bank
  ports:
  - port: 80 # Exposes the service on port 80
    targetPort: 80 #Pod's container Listens on 80, if you dont use this Sine, Kas will assign tila


apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: trainerreyaz/ib-image:latest
---
apiVersion: v1
kind: Service
metadata:
  name: ib-service
spec:
  type: NodePort
  selector:
    app: bank
  ports:
  - port: 80
    targetPort: 80      # Ensure this matches the container's port, if you dont use this line, K8s will assign default
    nodePort: 31433     # External access via <NodeIP>:31234 Must be in range 30000-32767

Client accesses http://<NodeIP>:31234

Traffic reaches Kubernetes Node on nodePort: 31234

The service forwards it to port: 80 (internal Service port)

Then it forwards the request to targetPort: 80 (inside the Pod's container)


apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: trainerreyaz/ib-image:latest
---
apiVersion: v1
kind: Service
metadata:
  name: ib-service
spec:
  type: LoadBalancer
  selector:
    app: bank
  ports:
  - port: 80
    targetPort: 80
    nodePort: 31433

----+++-----------------------------------------------------
Kubernetes Metric Server also called Heapster

kubectl apply f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability-

1.21+.yaml:

kubectl top pods. [Wait for 2 mins]

vi auto.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mb-deployment
  labels:
    app: bank
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank # Removed extra period
    spec:
      containers:
      - name: cont1 # Added hyphen list indicator
        image: reyadocker/mobilebankingrepo:latest 


kubectl create -f auto.yml

kubectl get pods

kubectl autoscale deployment mb-deployment --cpu-percent=20 --min=1--max=10

Autoscale deployment called mb-deployment if my cpu is more than 20%, scale out, min 1, max 10

kubectl get hpa [This wil show cpu: 1%/20%, now cpu percent is 1%, it will take time]

Now lets stress the pod by installing stress inside pod, lets connect to pod using exec and install stress.

kubectl get pods

kubectl exec it mb-deployment-85856755c5-c6fb7 -- /bin/bash
 
    apt update

    apt install stress

    stress-cpu 8-10 4-vm 2vm-bytes 128M-timeout 60s

    exit
cpu 8: Launches 8 CPU stressors, each consuming 100% CPU by performing continuous computations.

--io 4: Initiates 4 I/O stressors, each generating continuous 1/0 operations to stress the system's disk and filesystem.

--vm 2: Starts 2 virtual memory stressors, each allocating and deallocating memory repeatedly to test the system's memory management.

--vm-bytes 128M: Specifies that each virtual memory stressor should allocate 128 megabytes of memory.

--timeout 60s: Sets the duration of the stress test to 60 seconds, after which all stressors will terminate.

On main server

kubectl top pods

kubectl get hpa

kubectl get pods.

kubectl describe hpa mb-deployment [ This will show scaling activities ]

kubectl get events     [This will also show same things ]

kubectl logs mb-deployment-8585b755c5-p2bzv [To see logs of the pod]

After few mins, scale in happens as we dont have load.

kubectl delete -f auto.yml

Example using Manifestfile

First we need to have deployment and then we can autscale on that deployment / deployment name

vi auto.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 2
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: reyadocker/internetbankingrepo:latest

kubectl apply -f auto.yml

vi hpa.yml

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ib-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ib-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 20

----+++-----------------------------------------------------

Resource Quota:

kubectl create ns dev

kubectl config set-context --current --namespace=dev

kubectl config view [To see which namespace we are using]

vi dev-quota.yml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: dev
spec:
  hard:
    pods: "5"
    limits.cpu: "1"
    limits.memory: 1Gi

vi deploy1.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: reyadocker/internetbankingrepo:latest
        resources:
          limits:
            cpu: "0.3"
            memory: 320Mi


kubectl get pods
 kubectl get quota
----+++-----------------------------------------------------

PV Persistent Volume

PVC

TO use PV we need to claim the volume using PVC

PVC request a PV with your desired specification(size, access, modes & speed etc) from KBS and once a suitable PV is found it will bound to PVC

After bounding is done to pod you can mount is as a volume

Once user finished work, the attached PV can be released the underlying PV can be reclaimed and recycled for future

if you create volume in cluster, if cluster is delete storage is also deleted. SO use AWS EBS Volumes

First, create a EBS volume in EC2, with 20GB magnetic

kubectl delete ns dev [delete the namespace and come to default]

kubectl config set-context --current --namespace=default

vi pv.yml

apiVersion: v1
kind: PersistentVolume # Corrected name
metadata:
  name: my-pv
spec:
  capacity:
    storage: 20Gi # Corrected value and added unit
  accessModes:
  - ReadWriteOnce # Added hyphen list indicator and indentation
  awsElasticBlockStore:
    volumeID: vol-0771f0561f66408c9
    fsType: ext4


kubectl create -f pv.yml

kubectl get pv

Now create pvc

vi pvc.yml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi

kubectl create -f pvc.yml

vi deploy1.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: pvdeploy
  labels:
    app: bank
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: centos
        command: ["/bin/bash", "-c", "sleep", "10000"]
        volumeMounts:
        - name: my-pv
          mountPath: "/tmp/persistent"
      volumes:
      - name: my-pv
        persistentVolumeClaim:
          claimName: my-pvc

----+++-----------------------------------------------------
